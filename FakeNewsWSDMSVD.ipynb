{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/Users/Administrator/Downloads/FakeNewsWSDM/train.csv')\n",
    "test_df = pd.read_csv('/Users/Administrator/Downloads/FakeNewsWSDM/test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.append(test_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Administrator/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/Administrator/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_text_t1_en = df[['title1_en']] #+ test_df[['title1_en']]\n",
    "data_text_t1_en['index'] = df.index\n",
    "documents_t1 = data_text_t1_en\n",
    "\n",
    "data_text_t2_en = df[['title2_en']] #+ test_df[['title2_en']]\n",
    "data_text_t2_en['index'] = df.index\n",
    "documents_t2 = data_text_t2_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_text_t1_en = train_df[['title1_en']] #+ test_df[['title1_en']]\n",
    "#data_text_t1_en['index'] = train_df.index\n",
    "#documents_t1 = data_text_t1_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_text_t2_en = train_df[['title2_en']] #+ test_df[['title2_en']]\n",
    "#data_text_t2_en['index'] = train_df.index\n",
    "#documents_t2 = data_text_t2_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text_t1_en_test = test_df[['title1_en']]\n",
    "data_text_t1_en_test['index'] = test_df.index\n",
    "documents_t1_test = data_text_t1_en_test\n",
    "data_text_t2_en_test = test_df[['title2_en']]\n",
    "data_text_t2_en_test['index'] = test_df.index\n",
    "documents_t2_test = data_text_t2_en_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs = documents_t1['title1_en'] + documents_t2['title2_en']\n",
    "documents_t2.columns = ['title1_en', 'index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = documents_t1.append(documents_t2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_test = documents_t1_test['title1_en'] + documents_t2_test['title2_en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_merged = docs.append(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400678, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Administrator/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 [insur, benefit, peopl, rural, area]\n",
       "1    [come, shenzhen, sooner, later, come, year, sh...\n",
       "2    [come, shenzhen, sooner, later, come, year, sh...\n",
       "3    [come, shenzhen, sooner, later, come, year, sh...\n",
       "4                    [discrimin, gutter, mean, garlic]\n",
       "5    [come, shenzhen, sooner, later, come, year, sh...\n",
       "6                                [durian, kill, wrong]\n",
       "7    [come, shenzhen, sooner, later, come, year, sh...\n",
       "8              [frog, frog, fertil, test, play, jewel]\n",
       "9                    [discrimin, gutter, mean, garlic]\n",
       "Name: title1_en, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs_merged = docs['title1_en'].map(preprocess)\n",
    "processed_docs_merged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "processed_docs_merged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(processed_docs_merged)):\n",
    "    processed_docs_merged[i] = ' '.join(processed_docs_merged[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400678.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docs_merged)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " y = train_df.loc[:,'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new =[0] * len(y)\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 'unrelated':\n",
    "        y_new[i] = 0\n",
    "    elif y[i] == 'agreed':\n",
    "        y_new[i] = 1\n",
    "    elif y[i] == 'disagreed':\n",
    "        y_new[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(processed_docs_merged[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=100,         \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10)\n",
    "\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])\n",
    "\n",
    "svd_matrix = svd_transformer.fit_transform(processed_docs_merged[:400678])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True, ngram_range =(1,2))\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=100,         \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10)\n",
    "\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])\n",
    "\n",
    "svd_matrix = svd_transformer.fit_transform(processed_docs_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01918863,  0.0286159 ,  0.01808505, ...,  0.011052  ,\n",
       "         0.00617448,  0.02045532],\n",
       "       [ 0.02029697,  0.02127209,  0.01619079, ...,  0.00135781,\n",
       "        -0.02195548, -0.0591473 ],\n",
       "       [ 0.02029697,  0.02127209,  0.01619079, ...,  0.00135781,\n",
       "        -0.02195548, -0.0591473 ],\n",
       "       ...,\n",
       "       [ 0.00780833,  0.00961229,  0.00345061, ...,  0.01590715,\n",
       "        -0.00776811,  0.01904509],\n",
       "       [ 0.01671944,  0.01668049,  0.01201423, ..., -0.00030251,\n",
       "        -0.0029016 ,  0.00866314],\n",
       "       [ 0.00617216,  0.00717253,  0.00483158, ..., -0.01653337,\n",
       "         0.0034951 ,  0.02379681]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320552, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = svd_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320552, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text_t1_en_test = test_df[['title1_en']]\n",
    "data_text_t1_en_test['index'] = test_df.index\n",
    "documents_t1_test = data_text_t1_en_test\n",
    "data_text_t2_en_test = test_df[['title2_en']]\n",
    "data_text_t2_en_test['index'] = test_df.index\n",
    "documents_t2_test = data_text_t2_en_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = documents_t1_test['title1_en'] + documents_t2_test['title2_en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs_test = docs_test.map(preprocess)\n",
    "processed_docs_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(processed_docs_test)):\n",
    "    processed_docs_test[i] = ' '.join(processed_docs_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix_test = svd_transformer.fit_transform(processed_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09465792, 0.07959106, 0.0855641 , ..., 0.03435419, 0.00522916,\n",
       "        0.01169125],\n",
       "       [0.05635475, 0.04080039, 0.04651873, ..., 0.02997685, 0.04049267,\n",
       "        0.00927411],\n",
       "       [0.056504  , 0.03409208, 0.03553103, ..., 0.04189932, 0.04062652,\n",
       "        0.00884206],\n",
       "       ...,\n",
       "       [0.01180759, 0.00943695, 0.01055401, ..., 0.00482765, 0.00932629,\n",
       "        0.03539339],\n",
       "       [0.02676953, 0.02180736, 0.02464082, ..., 0.00247275, 0.00778814,\n",
       "        0.01658776],\n",
       "       [0.02465559, 0.02223488, 0.02647584, ..., 0.02951702, 0.00552991,\n",
       "        0.06344828]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(abs(X_train), y_train)\n",
    "\n",
    "# Fitting Logistic Regression to the Training set\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#classifier = LogisticRegression(random_state = None, multi_class = 'multinomial', class_weight = 'balanced', solver = 'newton-cg')\n",
    "#classifier.fit(X_train, y_train)\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "#classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MultinomialNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41410,  2520,     0],\n",
       "       [14906,  3608,     0],\n",
       "       [ 1646,    21,     0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11312, 21323, 11295],\n",
       "       [ 2421, 14985,  1108],\n",
       "       [  303,   167,  1197]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40064,  3771,    95],\n",
       "       [15459,  2959,    96],\n",
       "       [ 1580,    61,    26]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.91      0.79     43930\n",
      "          1       0.44      0.16      0.23     18514\n",
      "          2       0.12      0.02      0.03      1667\n",
      "\n",
      "avg / total       0.61      0.67      0.61     64111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.26      0.39     43930\n",
      "          1       0.41      0.81      0.55     18514\n",
      "          2       0.09      0.72      0.16      1667\n",
      "\n",
      "avg / total       0.67      0.43      0.43     64111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix_test =svd_matrix[320552:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(svd_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] != 0 and y_pred[i] != 1:\n",
    "        print(y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = [0] * len(y_pred)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        y_pred_final[i] ='unrelated'\n",
    "    elif y_pred[i] == 1:\n",
    "        y_pred_final[i] = 'agreed'\n",
    "    elif y_pred[i] == 2:\n",
    "        y_pred_final[i] = 'disagreed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred_final)):\n",
    "    if y_pred_final[i] != 'unrelated':\n",
    "        print(y_pred_final[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sub = [0] * len(y_pred)\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred_sub[i] = [0] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred_sub)):\n",
    "    y_pred_sub[i][0] = test_df.iloc[i,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred_sub)):\n",
    "    y_pred_sub[i][1] = y_pred_final[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(y_pred_sub, columns = [\"Id\", \"Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('/Users/Administrator/Downloads/FakeNewsWSDM/output4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
