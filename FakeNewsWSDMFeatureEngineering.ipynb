{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/Users/Administrator/Downloads/FakeNewsWSDM/train.csv')\n",
    "test_df = pd.read_csv('/Users/Administrator/Downloads/FakeNewsWSDM/test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Administrator/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "temp_df = train_df.ix[(train_df['label'] == 'disagreed'),['id', 'tid1','tid2', 'title1_zh', 'title2_zh','title1_en','title2_en','label']]\n",
    "\n",
    "for i in range(4):\n",
    "    temp_df = temp_df.append(temp_df,ignore_index=True)\n",
    "    \n",
    "temp_df_agreed = train_df.ix[(train_df['label'] == 'agreed'),['id', 'tid1','tid2', 'title1_zh', 'title2_zh','title1_en','title2_en','label']]\n",
    "    \n",
    "train_df = train_df.append(temp_df, ignore_index=True)\n",
    "train_df = train_df.append(temp_df_agreed, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545781"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Administrator/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_text_t1_en = train_df[['title1_en']] #+ test_df[['title1_en']]\n",
    "data_text_t1_en['index'] = train_df.index\n",
    "documents_t1 = data_text_t1_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Administrator/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_text_t2_en = train_df[['title2_en']] #+ test_df[['title2_en']]\n",
    "data_text_t2_en['index'] = train_df.index\n",
    "documents_t2 = data_text_t2_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_t2.columns = ['title1_en', 'index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = documents_t1.append(documents_t2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Administrator/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs_merged = docs['title1_en'].map(preprocess)\n",
    "\n",
    "t1_list = list(processed_docs_merged[:545781])\n",
    "t2_list = list(processed_docs_merged[545781:])\n",
    "\n",
    "t1_list_str = ['']*len(t1_list)\n",
    "t2_list_str = ['']*len(t2_list)\n",
    "for i in range(len(t1_list)):\n",
    "    t1_list_str[i] = ' '.join(t1_list[i])\n",
    "for i in range(len(t2_list)):\n",
    "    t2_list_str[i] = ' '.join(t2_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "s = 0\n",
    "count1 = 0\n",
    "count2 = 0 \n",
    "count3 = 0\n",
    "\n",
    "sim_agreed = [0]*len(train_df)\n",
    "sim_disagreed = [0]*len(train_df)\n",
    "sim_unrelated = [0]*len(train_df)\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    if train_df['label'][i]=='agreed' and t1_list_str[i] != '' and t2_list_str[i] != '' :\n",
    "        list = ['']*2\n",
    "        list[0] = t1_list_str[i]\n",
    "        list[1] = t2_list_str[i]  \n",
    "        m = vect.fit_transform(list).toarray()\n",
    "        sim_agreed[count1] = sum(np.multiply(m[0],m[1]))/len(m[0])\n",
    "        #s = s + sum(np.multiply(m[0],m[1]))/len(m[0])\n",
    "        count1 = count1+1\n",
    "        \n",
    "    elif train_df['label'][i]=='disagreed' and t1_list_str[i] != '' and t2_list_str[i] != '' :\n",
    "        list = ['']*2\n",
    "        list[0] = t1_list_str[i]\n",
    "        list[1] = t2_list_str[i]  \n",
    "        m = vect.fit_transform(list).toarray()\n",
    "        sim_disagreed[count2] = sum(np.multiply(m[0],m[1]))/len(m[0])\n",
    "        #s = s + sum(np.multiply(m[0],m[1]))/len(m[0])\n",
    "        count2 = count2+1\n",
    "        \n",
    "    elif train_df['label'][i]=='unrelated' and t1_list_str[i] != '' and t2_list_str[i] != '' :\n",
    "        list = ['']*2\n",
    "        list[0] = t1_list_str[i]\n",
    "        list[1] = t2_list_str[i]  \n",
    "        m = vect.fit_transform(list).toarray()\n",
    "        sim_unrelated[count3] = sum(np.multiply(m[0],m[1]))/len(m[0])\n",
    "        #s = s + sum(np.multiply(m[0],m[1]))/len(m[0])\n",
    "        count3 = count3+1\n",
    "    \n",
    "        \n",
    "sim_agreed = sim_agreed[:count1]\n",
    "sim_disagreed = sim_disagreed[:count2]\n",
    "sim_unrelated = sim_unrelated[:count3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_list = ['fake', 'deni', 'disclos', 'discourag', \n",
    "          'disinform', 'dismiss', 'dispel', 'fals', 'fool', 'myth', 'refut', 'report',\n",
    "           'respond', 'rumour', 'spread', 'truth']\n",
    "\n",
    "m1_list = [5, 2, 0.3, 0.3,2,1.5,0.3,2,1,5,3,0.3,0.3,5,0.3,0.3]\n",
    "\n",
    "count1 = 0\n",
    "count2 = 0 \n",
    "count3 = 0\n",
    "\n",
    "sim_agreed_m1 = [0]*len(train_df)\n",
    "sim_disagreed_m1 = [0]*len(train_df)\n",
    "sim_unrelated_m1 = [0]*len(train_df)\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    if train_df['label'][i]=='agreed' and t1_list_str[i] != '' and t2_list_str[i] != '' :\n",
    "        list1 = [0] * 16\n",
    "        list2 = [0] *16\n",
    "        for j in range(len(m_list)):\n",
    "            if(m_list[j] in t1_list_str[i]):\n",
    "                list1[j] = 1\n",
    "        for j in range(len(m_list)):\n",
    "            if(m_list[j] in t2_list_str[i]):\n",
    "                list2[j] = 1\n",
    "        sim_agreed_m1[count1] = sum(np.multiply(m1_list,list2)) - sum(np.multiply(m1_list,list1)) /len(m1_list)\n",
    "        #s = s + sum(np.multiply(m[0],m[1]))/len(m[0])\n",
    "        count1 = count1+1\n",
    "        \n",
    "    elif train_df['label'][i]=='disagreed' and t1_list_str[i] != '' and t2_list_str[i] != '' :\n",
    "        list1 = [0] * 16\n",
    "        list2 = [0] *16\n",
    "        for j in range(len(m_list)):\n",
    "            if(m_list[j] in t1_list_str[i]):\n",
    "                list1[j] = 1\n",
    "        for j in range(len(m_list)):\n",
    "            if(m_list[j] in t2_list_str[i]):\n",
    "                list2[j] = 1\n",
    "        sim_disagreed_m1[count2] = sum(np.multiply(m1_list,list2)) - sum(np.multiply(m1_list,list1)) /len(m1_list)\n",
    "        #s = s + sum(np.multiply(m[0],m[1]))/len(m[0])\n",
    "        count2 = count2+1\n",
    "        \n",
    "    elif train_df['label'][i]=='unrelated' and t1_list_str[i] != '' and t2_list_str[i] != '' :\n",
    "        list1 = [0] * 16\n",
    "        list2 = [0] *16\n",
    "        for j in range(len(m_list)):\n",
    "            if(m_list[j] in t1_list_str[i]):\n",
    "                list1[j] = 1\n",
    "        for j in range(len(m_list)):\n",
    "            if(m_list[j] in t2_list_str[i]):\n",
    "                list2[j] = 1\n",
    "        sim_unrelated_m1[count3] = sum(np.multiply(m1_list,list2)) - sum(np.multiply(m1_list,list1)) /len(m1_list)\n",
    "        #s = s + sum(np.multiply(m[0],m[1]))/len(m[0])\n",
    "        count3 = count3+1\n",
    "    \n",
    "        \n",
    "sim_agreed_m1 = sim_agreed_m1[:count1]\n",
    "sim_disagreed_m1 = sim_disagreed_m1[:count2]\n",
    "sim_unrelated_m1 = sim_unrelated_m1[:count3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sim_agreed_m1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ba23a8cfbda8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msim_agreed_m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sim_agreed_m1' is not defined"
     ]
    }
   ],
   "source": [
    "sim_agreed_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(sim_unrelated_m1) + len(sim_agreed_m1) + len(sim_disagreed_m1)\n",
    "X =[0] * l\n",
    "for i in range(l):\n",
    "    X[i] = [0] *3\n",
    "y =[0]*l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1 = 0\n",
    "count2 = 0 \n",
    "count3 = 0\n",
    "countj = 0\n",
    "\n",
    "l = len(sim_unrelated_m1) + len(sim_agreed_m1) + len(sim_disagreed_m1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    if train_df['label'][i]=='agreed' and t1_list_str[i] != '' and t2_list_str[i] != '' :\n",
    "        X[countj][0] = sim_agreed[count1] \n",
    "        X[countj][1] = sim_agreed_m1[count1]\n",
    "        X[countj][2] = sim[countj]\n",
    "        y[countj] = 1\n",
    "        count1 = count1+1\n",
    "        countj = countj+1\n",
    "        \n",
    "    elif train_df['label'][i]=='disagreed' and t1_list_str[i] != '' and t2_list_str[i] != '' :\n",
    "        X[countj][0] = sim_disagreed[count2] \n",
    "        X[countj][1] = sim_disagreed_m1[count2]\n",
    "        X[countj][2] = sim[countj]\n",
    "        y[countj] = 2\n",
    "        count2 = count2+1\n",
    "        countj = countj+1\n",
    "        \n",
    "    elif train_df['label'][i]=='unrelated' and t1_list_str[i] != '' and t2_list_str[i] != '' :\n",
    "        X[countj][0] = sim_unrelated[count3] \n",
    "        X[countj][1] = sim_unrelated_m1[count3]\n",
    "        X[countj][2] = sim[countj]\n",
    "        y[countj] = 0\n",
    "        count3 = count3+1\n",
    "        countj = countj+1\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=100,         \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10)\n",
    "\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])\n",
    "\n",
    "svd_matrix = svd_transformer.fit_transform(docs['title1_en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(545781, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_matrix[545781:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix_t1 = svd_matrix[:545781,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix_t2 = svd_matrix[545781:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sim = cosine_similarity(svd_matrix_t1, svd_matrix_t2, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix_t1[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(svd_matrix_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = [0] * len(svd_matrix_t1)\n",
    "for i in range(len(svd_matrix_t1)):\n",
    "    sim[i] = cosine_similarity(svd_matrix_t1[i,:].reshape(1,-1), svd_matrix_t2[i,:].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sim)):\n",
    "    sim[i] = sim[i][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " y = train_df.loc[:,'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new =[0] * len(y)\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 'unrelated':\n",
    "        y_new[i] = 0\n",
    "    elif y[i] == 'agreed':\n",
    "        y_new[i] = 1\n",
    "    elif y[i] == 'disagreed':\n",
    "        y_new[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_new = list(zip(sim, y_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(x_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy.random import randn\n",
    "#randn(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_df = pd.DataFrame(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(x_df, hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_df.columns = ['sim', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_df.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [0] * len(sim)\n",
    "for i in range(len(sim)):\n",
    "    X[i] = [sim[i],sim[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#X_train = sc.fit_transform(X_train)\n",
    "#X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fitting Logistic Regression to the Training set\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#classifier = LogisticRegression(random_state = None, multi_class = 'multinomial', class_weight = 'balanced', solver = 'newton-cg')\n",
    "#classifier.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25178, 10364,  8240],\n",
       "       [10195, 20367,  6583],\n",
       "       [ 7138,  6335, 14665]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.58      0.58     43782\n",
      "          1       0.55      0.55      0.55     37145\n",
      "          2       0.50      0.52      0.51     28138\n",
      "\n",
      "avg / total       0.55      0.55      0.55    109065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29730,  9511,  4541],\n",
       "       [ 9382, 27317,   446],\n",
       "       [ 9210,  9574,  9354]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.68      0.65     43782\n",
      "          1       0.59      0.74      0.65     37145\n",
      "          2       0.65      0.33      0.44     28138\n",
      "\n",
      "avg / total       0.62      0.61      0.60    109065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text_t1_en_test = test_df[['title1_en']] #+ test_df[['title1_en']]\n",
    "data_text_t1_en_test['index'] = test_df.index\n",
    "documents_t1_test = data_text_t1_en_test\n",
    "\n",
    "data_text_t2_en_test = test_df[['title2_en']] #+ test_df[['title2_en']]\n",
    "data_text_t2_en_test['index'] = test_df.index\n",
    "documents_t2_test = data_text_t2_en_test\n",
    "\n",
    "documents_t2_test.columns = ['title1_en', 'index']\n",
    "\n",
    "docs_test = documents_t1_test.append(documents_t2_test, ignore_index=True)\n",
    "\n",
    "svd_matrix_test = svd_transformer.fit_transform(docs_test['title1_en'])\n",
    "\n",
    "svd_matrix_t1_test = svd_matrix[:80126,:]\n",
    "\n",
    "svd_matrix_t2_test = svd_matrix[80126:,:]\n",
    "\n",
    "sim_test = [0] * len(svd_matrix_t1_test)\n",
    "for i in range(len(svd_matrix_t1_test)):\n",
    "    sim_test[i] = cosine_similarity(svd_matrix_t1_test[i,:].reshape(1,-1), svd_matrix_t2_test[i,:].reshape(1,-1))\n",
    "    \n",
    "for i in range(len(sim_test)):\n",
    "    sim_test[i] = sim_test[i][0][0]\n",
    "\n",
    "y_pred_test = [0]*len(sim_test)\n",
    "\n",
    "list_of_disagree_words = [\"fake\", \"rumour\", \"rumor\", \"myth\", \"refute\", \n",
    "                          \"refut\", \"truth is\", \"wrong\", \"false\", \"gossip\", \"made up\"]\n",
    "for i in range(len(sim_test)):\n",
    "        \n",
    "        if sim_test[i] < 0.5:\n",
    "            y_pred_test[i] = 0\n",
    "            \n",
    "        elif any(word in (test_df['title1_en'][i]).lower() for word in list_of_disagree_words) and any(word in (test_df['title2_en'][i]).lower() for word in list_of_disagree_words):\n",
    "                y_pred_test[i] = 2\n",
    "        \n",
    "        else:\n",
    "            y_pred_test[i] = 1\n",
    "            \n",
    "            \n",
    "                \n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in range(len(y_pred_test)):\n",
    "    if y_pred_test[i] == 2:\n",
    "        j = j+1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = [0] * len(y_pred_test)\n",
    "for i in range(len(y_pred_test)):\n",
    "    if y_pred_test[i] == 0:\n",
    "        y_pred_final[i] ='unrelated'\n",
    "    elif y_pred_test[i] == 1:\n",
    "        y_pred_final[i] = 'agreed'\n",
    "    elif y_pred_test[i] == 2:\n",
    "        y_pred_final[i] = 'disagreed'\n",
    "\n",
    "\n",
    "y_pred_sub = [0] * len(y_pred_test)\n",
    "for i in range(len(y_pred_test)):\n",
    "    y_pred_sub[i] = [0] * 2\n",
    "    \n",
    "for i in range(len(y_pred_sub)):\n",
    "    y_pred_sub[i][0] = test_df.iloc[i,0]\n",
    "    \n",
    "for i in range(len(y_pred_sub)):\n",
    "    y_pred_sub[i][1] = y_pred_final[i]\n",
    "    \n",
    "out = pd.DataFrame(y_pred_sub, columns = [\"Id\", \"Category\"])\n",
    "\n",
    "out.to_csv('/Users/Administrator/Downloads/FakeNewsWSDM/output7.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_df['title1_en'][1]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_disagree_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(sim_test)):\n",
    "        if any(word in (test_df['title1_en'][i]).lower() for word in list_of_disagree_words) or any(word in (test_df['title2_en'][i]).lower() for word in list_of_disagree_words):\n",
    "            print(test_df['title1_en'][i] + '\\t')\n",
    "            print(test_df['title2_en'][i] + '\\n')\n",
    "            print(sim_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean((sim_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean((sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sim_test)):\n",
    "        if sim_test[i] == np.max(sim_test):\n",
    "            print(test_df['title1_en'][i] + '\\t')\n",
    "            print(test_df['title2_en'][i] + '\\n')\n",
    "            print(sim_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
